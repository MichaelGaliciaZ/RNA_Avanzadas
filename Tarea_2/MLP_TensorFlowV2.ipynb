{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e215834",
   "metadata": {},
   "source": [
    "# Tarea 2: Red Neuronal Perceptrón Multicapa con TensorFlow V2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bc1cb0",
   "metadata": {},
   "source": [
    "## Cargamos las librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5265182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets.fashion_mnist import load_data\n",
    "fashion_mnist = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3055152f",
   "metadata": {},
   "source": [
    "## Extraemos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4403262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bc1741",
   "metadata": {},
   "source": [
    "## Union de Datos\n",
    "\n",
    "El dataset Fashion_Mnist cuenta con 60,000 imagenes de entrenamiento y 10,000 imagenes de prueba; Para los fines de esta tarea, se hara una división del dataset con una relación 30-70, es decir, %30 del dataset será destinado a test y %70 a entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e5507",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((x_train, x_test))\n",
    "Y = np.concatenate((y_train, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b596941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(len(X), 28*28).astype('float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f0cd93",
   "metadata": {},
   "source": [
    "## Codificación One Hot Encoding \n",
    "\n",
    "Las clases se representan con etiquetas numericas en el rango de $0,\\cdots,9$:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Identificador \t| Clase \t|\n",
    "|---\t|---\t|\n",
    "| 0 \t| T-shirt/top \t|\n",
    "| 1 \t| Trouser \t|\n",
    "| 2 \t| Pullover \t|\n",
    "| 3 \t| Dress \t|\n",
    "| 4 \t| Coat \t|\n",
    "| 5 \t| Sandal \t|\n",
    "| 6 \t| Shirt \t|\n",
    "| 7 \t| Sneaker \t|\n",
    "| 8 \t| Bag \t|\n",
    "| 9 \t| Ankle Boot \t|\n",
    "\n",
    "</div>\n",
    "\n",
    "Debido al identificador puede (o no) realizarse una codificación del tipo One Hot. Esta codificación permite que el modelo neuronal interprete la distancia entre identificadores de la misma forma, es decir, en un mal aprendizaje puede asociar la clase \"Trouser\" con la clase \"Pullover\" por que los identificadores son cercanos entre si, de forma comtraria es posible que interprete una relación (erronea) entre los identificadores mas separadas, lo cual no es relevante.\n",
    "\n",
    "A fin de observar el efecto de la codificación se realizaran dos modelos: uno con codificación ONE HOT y otro sin ella.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df244be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_HOT_ENC = OneHotEncoder(sparse=False)\n",
    "Y_OHE = Y.reshape(len(Y), 1)\n",
    "Y_OHE = ONE_HOT_ENC.fit_transform(Y_OHE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef40689",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Las dimensiones del vector de etiquetas son {}\".format(Y.shape))\n",
    "print(\"Los primeros 10 elementos son: \\n {}\".format(Y[0:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affde012",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Los primeros 10 elementos codificados son: \\n {}\".format(Y_OHE[0:10]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b556b83e",
   "metadata": {},
   "source": [
    "## Arquitectura de el modelo\n",
    "\n",
    "Se aprovecha el paradigma de la programación orientada a objetos de Python para desarrollar una instancia que permita la declaración de la arquitectura del modelo MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc969293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_Model():\n",
    "    def __init__(self, lenght_input, lenght_output):\n",
    "        self.Layer_1_Weights = tf.Variable(np.random.rand(lenght_input, 700),\n",
    "                                           name=\"Layer_1_Weights\",\n",
    "                                           dtype=\"float32\")\n",
    "        self.Layer_1_Bias = tf.Variable(np.random.rand(700),\n",
    "                                        name=\"Layer_1_Bias\",\n",
    "                                        dtype=\"float\")\n",
    "        self.Layer_2_Weights = tf.Variable(np.random.rand(700, 650),\n",
    "                                           name=\"Layer_2_Weights\",\n",
    "                                           dtype=\"float32\")\n",
    "        self.Layer_2_Bias = tf.Variable(np.random.rand(650),\n",
    "                                        name=\"Layer_2_Bias\",\n",
    "                                        dtype=\"float\")\n",
    "        self.Layer_3_Weights = tf.Variable(np.random.rand(650, 500),\n",
    "                                           name=\"Layer_3_Weights\",\n",
    "                                           dtype=\"float32\")\n",
    "        self.Layer_3_Bias = tf.Variable(np.random.rand(500),\n",
    "                                        name=\"Layer_3_Bias\",\n",
    "                                        dtype=\"float\")\n",
    "        self.Output_Layer_Weights = tf.Variable(np.random.rand(500, lenght_output),\n",
    "                                                name=\"Output_Layer_Weights\",\n",
    "                                                dtype=\"float32\")\n",
    "        self.Output_Layer_Bias = tf.Variable(np.random.rand(lenght_output),\n",
    "                                             name=\"Output_Bias\",\n",
    "                                             dtype=\"float32\")\n",
    "        self.Trainable_Variables = [self.Layer_1_Weights,\n",
    "                                    self.Layer_1_Bias,\n",
    "                                    self.Layer_2_Weights,\n",
    "                                    self.Layer_2_Bias,\n",
    "                                    self.Layer_3_Weights,\n",
    "                                    self.Layer_3_Bias,\n",
    "                                    self.Output_Layer_Weights,\n",
    "                                    self.Output_Layer_Bias]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        L1 = tf.add(tf.matmul(x, self.Layer_1_Weights), self.Layer_1_Bias)\n",
    "        L1 = tf.nn.relu(L1)\n",
    "        L2 = tf.add(tf.matmul(L1, self.Layer_2_Weights), self.Layer_2_Bias)\n",
    "        L2 = tf.nn.relu(L2)\n",
    "        L3 = tf.add(tf.matmul(L2, self.Layer_3_Weights), self.Layer_3_Bias)\n",
    "        L3 = tf.nn.relu(L3)\n",
    "\n",
    "        OutPut = tf.matmul(L3, self.Output_Layer_Weights) + \\\n",
    "            self.Output_Layer_Bias\n",
    "\n",
    "        return OutPut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b034d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizador = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628464d6",
   "metadata": {},
   "source": [
    "## Configuración del gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671757d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, tdata, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(tdata)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels, predictions))\n",
    "    gradients = tape.gradient(loss, model.Trainable_Variables)\n",
    "    capped_grads_and_vars = [(grad, model.Trainable_Variables[index])\n",
    "                             for index, grad in enumerate(gradients)]\n",
    "    optimizador.apply_gradients(capped_grads_and_vars)\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e6d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(model, tdata, labels):\n",
    "    predictions = model(tdata)\n",
    "    t_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels, predictions))\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitting(model, train_x, train_y, test_x, test_y, EPOCHS, N_batch, batch_size):\n",
    "    Train_L = []\n",
    "    Test_L = []\n",
    "    Train_A = []\n",
    "    Test_A = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        i = 0\n",
    "        while i+batch_size < len(train_x) or i+batch_size < batch_size*N_batch:\n",
    "            start = i\n",
    "            end = i+batch_size\n",
    "            batch_x = train_x[start:end]\n",
    "            batch_y = train_y[start:end]\n",
    "            train_step(model, batch_x, batch_y)\n",
    "            i += batch_size\n",
    "        test_step(model, test_x, test_y)\n",
    "\n",
    "        template = 'Epoch {}, Perdida: {}, Exactitud: {}, Perdida de prueba: {}, Exactitud de prueba: {}'\n",
    "        print(template.format(epoch+1,\n",
    "                              train_loss.result(),\n",
    "                              train_accuracy.result()*100,\n",
    "                              test_loss.result(),\n",
    "                              test_accuracy.result()*100))\n",
    "        Train_L.append(train_loss.result())\n",
    "        Train_A.append(train_accuracy.result())\n",
    "        Test_L.append(test_loss.result())\n",
    "        Test_A.append(test_accuracy.result())\n",
    "\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        test_loss.reset_states()\n",
    "        test_accuracy.reset_states()\n",
    "\n",
    "    return (Train_A, Test_A), (Train_L, Test_L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a3ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN = DNN_Model(lenght_input=28*28, lenght_output=10)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y_OHE, train_size=0.8, random_state=2022)\n",
    "\n",
    "print(\"Tamaño del set de entrenamiento: {} elementos\".format(len(x_train)))\n",
    "print(\"Tamaño del set de prueba: {} elementos\".format(len(x_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5548008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"Epocas\": 25,\n",
    "          \"Numero Batches\": 100,\n",
    "          \"Tamaño de Batch\": 80}\n",
    "\n",
    "(Train_A, Test_A), (Train_L, Test_L) = fitting(DNN, x_train, y_train, x_test, y_test, *params.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c5a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(Train_A)\n",
    "plt.plot(Test_A)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11529923",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Train_L)\n",
    "plt.plot(Test_L)\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b2508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN.save('MAGZ_FMNist.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('CIC_GPU')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "b4dee9c77bc81e8bc3192f031245e7af43a1d4d0287b5c55b4b36e62661d36f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
